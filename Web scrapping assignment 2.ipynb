{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "html_text = requests.get(\"https://in.indeed.com/jobs?q=data%20analyst&l=Bangalore%2C%20Karnataka\").text\n",
    "soup = BeautifulSoup(html_text,'html.parser')\n",
    "jobs = soup.find_all('div',class_='jobsearch-SerpJobCard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = []\n",
    "company = []\n",
    "summary = []\n",
    "job_location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs:\n",
    "    atag = job.h2.a\n",
    "    job_title = atag.get('title')\n",
    "    job_name.append(job_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in jobs:\n",
    "    ctag = x.find('span','company').text.strip()\n",
    "    company.append(ctag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in jobs:\n",
    "    place = location.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    job_location.append(place)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in jobs:\n",
    "    sm = i.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    summary.append(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>EXPECTATIONS</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Return Data Reconciliation Analyst</td>\n",
       "      <td>TPS Technologies</td>\n",
       "      <td>You should have a passion for data analysis an...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst (1-2 Yrs)</td>\n",
       "      <td>CommerceIQ</td>\n",
       "      <td>Business analyst: 1 year (Required). We are lo...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Uplatz</td>\n",
       "      <td>Thoroughly understand the underlying data, bus...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Analyst Intern</td>\n",
       "      <td>Neukelp Innovation Technology Pvt. Ltd.</td>\n",
       "      <td>The ideal candidate is a team player who will ...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Associate, Data Analysis</td>\n",
       "      <td>Fiserv, Inc.</td>\n",
       "      <td>Leverage tools to analyze data and derive insi...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Autodesk</td>\n",
       "      <td>Share data availability and data quality, iden...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Business Analyst- HR Data Collection/Ingestion</td>\n",
       "      <td>Société Générale</td>\n",
       "      <td>Must be comfortable getting into details on da...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Professional, Data Analysis</td>\n",
       "      <td>Fiserv, Inc.</td>\n",
       "      <td>Expert in Meta data modeling and Reports devel...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AWM - Client Data - Analyst</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>J.P. Morgan Asset &amp; Wealth Management, with cl...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MIS Reporting Analyst</td>\n",
       "      <td>DayToDay Health</td>\n",
       "      <td>Perform data analysis for generating reports o...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        JOB_TITLE  \\\n",
       "0              Return Data Reconciliation Analyst   \n",
       "1                      Business Analyst (1-2 Yrs)   \n",
       "2                                    Data Analyst   \n",
       "3                         Business Analyst Intern   \n",
       "4                        Associate, Data Analysis   \n",
       "5                                    Data Analyst   \n",
       "6  Business Analyst- HR Data Collection/Ingestion   \n",
       "7                     Professional, Data Analysis   \n",
       "8                     AWM - Client Data - Analyst   \n",
       "9                           MIS Reporting Analyst   \n",
       "\n",
       "                                   COMPANY  \\\n",
       "0                         TPS Technologies   \n",
       "1                               CommerceIQ   \n",
       "2                                   Uplatz   \n",
       "3  Neukelp Innovation Technology Pvt. Ltd.   \n",
       "4                             Fiserv, Inc.   \n",
       "5                                 Autodesk   \n",
       "6                         Société Générale   \n",
       "7                             Fiserv, Inc.   \n",
       "8                JPMorgan Chase Bank, N.A.   \n",
       "9                          DayToDay Health   \n",
       "\n",
       "                                        EXPECTATIONS              LOCATION  \n",
       "0  You should have a passion for data analysis an...  Bengaluru, Karnataka  \n",
       "1  Business analyst: 1 year (Required). We are lo...  Bengaluru, Karnataka  \n",
       "2  Thoroughly understand the underlying data, bus...  Bengaluru, Karnataka  \n",
       "3  The ideal candidate is a team player who will ...  Bengaluru, Karnataka  \n",
       "4  Leverage tools to analyze data and derive insi...  Bengaluru, Karnataka  \n",
       "5  Share data availability and data quality, iden...  Bengaluru, Karnataka  \n",
       "6  Must be comfortable getting into details on da...  Bengaluru, Karnataka  \n",
       "7  Expert in Meta data modeling and Reports devel...  Bengaluru, Karnataka  \n",
       "8  J.P. Morgan Asset & Wealth Management, with cl...  Bengaluru, Karnataka  \n",
       "9  Perform data analysis for generating reports o...  Bengaluru, Karnataka  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "weather = pd.DataFrame({\n",
    "        \"JOB_TITLE\": job_name[:10],\n",
    "         \"COMPANY\": company[:10],\n",
    "         \"EXPECTATIONS\": summary[:10],\n",
    "         \"LOCATION\": job_location[:10]\n",
    "         \n",
    "    })\n",
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "html_text1 = requests.get(\"https://in.indeed.com/jobs?q=data%20scientist&l=Bangalore%2C%20Karnataka&advn=7641058883739924&vjk=6dc1a9b79905765e\").text\n",
    "soup1 = BeautifulSoup(html_text1,'html.parser')\n",
    "jobs1 = soup1.find_all('div',class_='jobsearch-SerpJobCard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "Company = []\n",
    "Summary = []\n",
    "location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs1:\n",
    "    btag = i.h2.a\n",
    "    title = btag.get('title')\n",
    "    name.append(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in jobs1:\n",
    "    Ctag = x.find('span','company').text.strip()\n",
    "    Company.append(Ctag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pla in jobs1:\n",
    "    place = pla.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    location.append(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in jobs1:\n",
    "    sam = i.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    Summary.append(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>EXPECTATIONS</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Uplatz</td>\n",
       "      <td>Uplatz is hiring Fresher Data Scientist. Under...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Rinteger</td>\n",
       "      <td>(2)Processing, cleansing, and verifying the in...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist I</td>\n",
       "      <td>Caterpillar</td>\n",
       "      <td>This person is proficient on Transportation Ma...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Scientist (4-6 yrs)</td>\n",
       "      <td>CommerceIQ</td>\n",
       "      <td>Work with large scale ecommerce data of the bi...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist I</td>\n",
       "      <td>Honeywell</td>\n",
       "      <td>Deliver business value through Right and Fast ...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WM Data Scientist</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>Advanced knowledge of data, application and in...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mast Global</td>\n",
       "      <td>Query structured/unstructured data and perform...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Eton Solutions</td>\n",
       "      <td>ETON Solutions is a third generation wealth ma...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>RTEX HR Services Pvt. Ltd</td>\n",
       "      <td>Role &amp; Responsibilities- Data Scientist/Data A...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Staff Data Scientist</td>\n",
       "      <td>Walmart Global Tech India</td>\n",
       "      <td>We’re a team of 15,000+ software engineers, da...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         JOB_TITLE                    COMPANY  \\\n",
       "0                   Data Scientist                     Uplatz   \n",
       "1                   Data Scientist                   Rinteger   \n",
       "2                 Data Scientist I                Caterpillar   \n",
       "3  Senior Data Scientist (4-6 yrs)                 CommerceIQ   \n",
       "4                 Data Scientist I                  Honeywell   \n",
       "5                WM Data Scientist  JPMorgan Chase Bank, N.A.   \n",
       "6                   Data Scientist                Mast Global   \n",
       "7                   Data Scientist             Eton Solutions   \n",
       "8                   Data Scientist  RTEX HR Services Pvt. Ltd   \n",
       "9             Staff Data Scientist  Walmart Global Tech India   \n",
       "\n",
       "                                        EXPECTATIONS              LOCATION  \n",
       "0  Uplatz is hiring Fresher Data Scientist. Under...  Bengaluru, Karnataka  \n",
       "1  (2)Processing, cleansing, and verifying the in...  Bengaluru, Karnataka  \n",
       "2  This person is proficient on Transportation Ma...  Bengaluru, Karnataka  \n",
       "3  Work with large scale ecommerce data of the bi...  Bengaluru, Karnataka  \n",
       "4  Deliver business value through Right and Fast ...  Bengaluru, Karnataka  \n",
       "5  Advanced knowledge of data, application and in...  Bengaluru, Karnataka  \n",
       "6  Query structured/unstructured data and perform...  Bengaluru, Karnataka  \n",
       "7  ETON Solutions is a third generation wealth ma...  Bengaluru, Karnataka  \n",
       "8  Role & Responsibilities- Data Scientist/Data A...  Bengaluru, Karnataka  \n",
       "9  We’re a team of 15,000+ software engineers, da...  Bengaluru, Karnataka  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "qwe = pd.DataFrame({\n",
    "        \"JOB_TITLE\": name[:10],\n",
    "         \"COMPANY\": Company[:10],\n",
    "         \"EXPECTATIONS\": Summary[:10],\n",
    "         \"LOCATION\": location[:10]\n",
    "    })\n",
    "qwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "Scrape data for “Data Scientist” designation for first 10 job results from Naukri.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text2 = requests.get(\"https://in.indeed.com/jobs?q=data%20scientist%20%E2%82%B93%2C00%2C000&l=Delhi&ts=1613473497941&rq=1&rsIdx=1\").text\n",
    "soup2 = BeautifulSoup(html_text2,'html.parser')\n",
    "jobs2 = soup2.find_all('div',class_='jobsearch-SerpJobCard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = []\n",
    "Company_name = []\n",
    "Summary = []\n",
    "Location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs2:\n",
    "    ytag = i.h2.a\n",
    "    title = ytag.get('title')\n",
    "    job_name.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in jobs2:\n",
    "    otag = x.find('span','company').text.strip()\n",
    "    Company_name.append(otag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pl in jobs2:\n",
    "    Place = pl.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    Location.append(Place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs2:\n",
    "    sym = i.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    Summary.append(sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>EXPECTATIONS</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ESRI, Inc.</td>\n",
       "      <td>Our team develops tools, APIs, and AI models f...</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Ohmyhome</td>\n",
       "      <td>Work with data and analytics experts to strive...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>GreenTech Intelligent Transportation System LLP</td>\n",
       "      <td>Good programming skills in Python/C++. Experie...</td>\n",
       "      <td>Kalkaji, Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>The BCG GAMMA team is comprised of world-class...</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DATA SCIENTIST</td>\n",
       "      <td>Innefu Labs Pvt. Ltd.</td>\n",
       "      <td>Keep up-to-date with latest technology trends....</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sr.Data Scientist</td>\n",
       "      <td>Innefu Labs Pvt. Ltd.</td>\n",
       "      <td>Must be able to handle deployment complexities...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Ank Aha</td>\n",
       "      <td>Ensure data integrity and consistency across r...</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Manager Data Scientist</td>\n",
       "      <td>Innefu Labs Pvt. Ltd.</td>\n",
       "      <td>Must be able to handle deployment complexities...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Fitfyles</td>\n",
       "      <td>To perform data analyses on massive data sets,...</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Deep Learning / Computer Vision Engineer</td>\n",
       "      <td>Agrex Technologies Private Limited</td>\n",
       "      <td>Understanding of data structures, data modelin...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  JOB_TITLE  \\\n",
       "0                            Data Scientist   \n",
       "1                             Data Engineer   \n",
       "2                     Senior Data Scientist   \n",
       "3                            Data Scientist   \n",
       "4                            DATA SCIENTIST   \n",
       "5                         Sr.Data Scientist   \n",
       "6                            Data Scientist   \n",
       "7                    Manager Data Scientist   \n",
       "8                            Data Scientist   \n",
       "9  Deep Learning / Computer Vision Engineer   \n",
       "\n",
       "                                           COMPANY  \\\n",
       "0                                       ESRI, Inc.   \n",
       "1                                         Ohmyhome   \n",
       "2  GreenTech Intelligent Transportation System LLP   \n",
       "3                          Boston Consulting Group   \n",
       "4                            Innefu Labs Pvt. Ltd.   \n",
       "5                            Innefu Labs Pvt. Ltd.   \n",
       "6                                          Ank Aha   \n",
       "7                            Innefu Labs Pvt. Ltd.   \n",
       "8                                         Fitfyles   \n",
       "9               Agrex Technologies Private Limited   \n",
       "\n",
       "                                        EXPECTATIONS               LOCATION  \n",
       "0  Our team develops tools, APIs, and AI models f...       New Delhi, Delhi  \n",
       "1  Work with data and analytics experts to strive...           Delhi, Delhi  \n",
       "2  Good programming skills in Python/C++. Experie...  Kalkaji, Delhi, Delhi  \n",
       "3  The BCG GAMMA team is comprised of world-class...       New Delhi, Delhi  \n",
       "4  Keep up-to-date with latest technology trends....           Delhi, Delhi  \n",
       "5  Must be able to handle deployment complexities...           Delhi, Delhi  \n",
       "6  Ensure data integrity and consistency across r...       New Delhi, Delhi  \n",
       "7  Must be able to handle deployment complexities...           Delhi, Delhi  \n",
       "8  To perform data analyses on massive data sets,...       New Delhi, Delhi  \n",
       "9  Understanding of data structures, data modelin...           Delhi, Delhi  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "twe = pd.DataFrame({\n",
    "        \"JOB_TITLE\": job_name[:10],\n",
    "         \"COMPANY\": Company_name[:10],\n",
    "         \"EXPECTATIONS\": Summary[:10],\n",
    "         \"LOCATION\": Location[:10]\n",
    "    })\n",
    "twe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\n",
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "Brand\n",
    "Product Description\n",
    "Price\n",
    "Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand1 = []\n",
    "Product_Description1 = []\n",
    "Price1 = []\n",
    "Discount1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pags = list(range(1,5))\n",
    "for page in pags:\n",
    "    requr = requests.get(\"https://www.flipkart.com/search?q=sunglasses&sid=26x&as=on&as-show=on&otracker=AS_QueryStore_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_QueryStore_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sunglasses%7CSunglasses&requestId=ed6f54e5-30b5-43c9-80e9-476306844180&as-searchtext=sung\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup9 = BeautifulSoup(requr,'html.parser')\n",
    "    des = soup9.find_all('div' , class_='_2WkVRV')\n",
    "    for i in range(len(des)):\n",
    "        Brand1.append(des[i].text)\n",
    "    len(Brand1)\n",
    "    price = soup9.find_all('div',class_='_30jeq3') \n",
    "    for i in range(len(price)):\n",
    "        Price1.append(price[i].text)\n",
    "        len(Price1)\n",
    "    xc = soup9.find_all('a',class_='IRpwTa') \n",
    "    for i in range(len(xc)):\n",
    "        Product_Description1.append(xc[i].text)\n",
    "        len(xc)\n",
    "    cv = soup9.find_all('div',class_='_3Ay6Sb')\n",
    "    for i in range(len(cv)):\n",
    "        Discount1.append(cv[i].text)\n",
    "        len(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESC</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹197</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (50)</td>\n",
       "      <td>₹314</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹758</td>\n",
       "      <td>15% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹666</td>\n",
       "      <td>16% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹225</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹314</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹399</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>UV Protection Round Sunglasses (Free Size)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (50)</td>\n",
       "      <td>₹246</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             BRAND                                       PRODUCT_DESC PRICE  \\\n",
       "0           PIRASO              UV Protection Aviator Sunglasses (54)  ₹197   \n",
       "1           PIRASO              UV Protection Aviator Sunglasses (50)  ₹314   \n",
       "2         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ₹758   \n",
       "3         Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹666   \n",
       "4           PIRASO              UV Protection Aviator Sunglasses (54)  ₹225   \n",
       "..             ...                                                ...   ...   \n",
       "95        Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...  ₹499   \n",
       "96  ROZZETTA CRAFT  UV Protection, Gradient Rectangular Sunglasses...  ₹314   \n",
       "97    Singco India       UV Protection Aviator Sunglasses (Free Size)  ₹399   \n",
       "98    Singco India         UV Protection Round Sunglasses (Free Size)  ₹499   \n",
       "99          PIRASO              UV Protection Aviator Sunglasses (50)  ₹246   \n",
       "\n",
       "   DISCOUNT  \n",
       "0   87% off  \n",
       "1   80% off  \n",
       "2   15% off  \n",
       "3   16% off  \n",
       "4   85% off  \n",
       "..      ...  \n",
       "95  50% off  \n",
       "96  80% off  \n",
       "97  80% off  \n",
       "98  77% off  \n",
       "99  83% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "kwe = pd.DataFrame({\n",
    "        \"BRAND\": Brand1[:100],\n",
    "         \"PRODUCT_DESC\": Product_Description1[:100],\n",
    "         \"PRICE\": Price1[:100],\n",
    "         \"DISCOUNT\": Discount1[:100]\n",
    "    })\n",
    "kwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.¶\n",
    "Scrape 100 reviews data from flipkart.com for iphone11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating = []\n",
    "Review = []\n",
    "Review_Summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pag = list(range(1,12))\n",
    "for page in pag:\n",
    "    reque = requests.get(\"https://www.flipkart.com/apple-iphone-11-white-64-gb/product-reviews/itmfc6a7091eb20b?pid=MOBFWQ6BVWVEH3XE&lid=LSTMOBFWQ6BVWVEH3XESAHPTP&marketplace=FLIPKART\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup6 = BeautifulSoup(reque,'html.parser')\n",
    "    qwer = soup6.find_all('div' , class_='_3LWZlK _1BLPMq')\n",
    "    for i in range(len(qwer)):\n",
    "        Rating.append(qwer[i].text)\n",
    "    len(Rating)\n",
    "    rev = soup6.find_all('p',class_='_2-N8zT') \n",
    "    for i in range(len(rev)):\n",
    "        Review.append(rev[i].text)\n",
    "        len(Review)\n",
    "    rs = soup6.find_all('div',class_='t-ZTKy') \n",
    "    for i in range(len(rs)):\n",
    "        Review_Summary.append(rs[i].text)\n",
    "        len(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RATING</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>REVIEW_SUMMARY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the MoneyThe iPhone 11 offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.I’m am ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money❤️❤️Its awesome mobile phone in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>*Review after 10 months of usage*Doesn't seem ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Best budget Iphone till date ❤️ go for it guys...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RATING              REVIEW  \\\n",
       "0       5           Brilliant   \n",
       "1       5    Perfect product!   \n",
       "2       5   Worth every penny   \n",
       "3       5       Great product   \n",
       "4       5  Highly recommended   \n",
       "..    ...                 ...   \n",
       "95      5    Perfect product!   \n",
       "96      5    Perfect product!   \n",
       "97      5           Fabulous!   \n",
       "98      5           Wonderful   \n",
       "99      5   Worth every penny   \n",
       "\n",
       "                                       REVIEW_SUMMARY  \n",
       "0   The Best Phone for the MoneyThe iPhone 11 offe...  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Previously I was using one plus 3t it was a gr...  \n",
       "3   Amazing Powerful and Durable Gadget.I’m am ver...  \n",
       "4   iphone 11 is a very good phone to buy only if ...  \n",
       "..                                                ...  \n",
       "95  It’s a must buy who is looking for an upgrade ...  \n",
       "96  Value for money❤️❤️Its awesome mobile phone in...  \n",
       "97  This is my first iOS phone. I am very happy wi...  \n",
       "98  *Review after 10 months of usage*Doesn't seem ...  \n",
       "99  Best budget Iphone till date ❤️ go for it guys...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pwe = pd.DataFrame({\n",
    "        \"RATING\": Rating[:100],\n",
    "         \"REVIEW\": Review[:100],\n",
    "         \"REVIEW_SUMMARY\": Review_Summary[:100],\n",
    "    })\n",
    "pwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\n",
    "Scrape data for first 100 sneakers you find when you visit flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "discount = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pags = list(range(1,5))\n",
    "for page in pags:\n",
    "    reqe = requests.get(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup5 = BeautifulSoup(reqe,'html.parser')\n",
    "    descr = soup5.find_all('div' , class_='_2WkVRV')\n",
    "    for i in range(len(descr)):\n",
    "        Brand.append(descr[i].text)\n",
    "    len(Brand)\n",
    "    price = soup5.find_all('div',class_='_30jeq3') \n",
    "    for i in range(len(price)):\n",
    "        Price.append(price[i].text)\n",
    "        len(Price)\n",
    "    xc = soup5.find_all('a',class_='IRpwTa') \n",
    "    for i in range(len(xc)):\n",
    "        Product_Description.append(xc[i].text)\n",
    "        len(xc)\n",
    "    cv = soup5.find_all('div',class_='_3Ay6Sb')\n",
    "    for i in range(len(cv)):\n",
    "        discount.append(cv[i].text)\n",
    "        len(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESC</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENJOY the celebration people</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹649</td>\n",
       "      <td>35% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bucik</td>\n",
       "      <td>Multicolor Casual Shoes Mesh for Men Sneakers ...</td>\n",
       "      <td>₹599</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Perfect &amp; Affordable Combo Pack of 02 Pairs Sn...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>TR</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹298</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>HOTSTYLE</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹474</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Ktiz</td>\n",
       "      <td>Rockstyle Trending Multicolor Ultralight canva...</td>\n",
       "      <td>₹398</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Men's Combo Pack of 02 Shoes for Men Casual Sn...</td>\n",
       "      <td>₹470</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Bonexy</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹664</td>\n",
       "      <td>71% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           BRAND  \\\n",
       "0   ENJOY the celebration people   \n",
       "1                          Bucik   \n",
       "2                         Chevit   \n",
       "3                   Robbie jones   \n",
       "4                         Chevit   \n",
       "..                           ...   \n",
       "95                            TR   \n",
       "96                      HOTSTYLE   \n",
       "97                          Ktiz   \n",
       "98                        Chevit   \n",
       "99                        Bonexy   \n",
       "\n",
       "                                         PRODUCT_DESC PRICE DISCOUNT  \n",
       "0                                    Sneakers For Men  ₹649  35% off  \n",
       "1   Multicolor Casual Shoes Mesh for Men Sneakers ...  ₹599  70% off  \n",
       "2   Perfect & Affordable Combo Pack of 02 Pairs Sn...  ₹499  72% off  \n",
       "3      Casual Sneakers Shoes For Men Sneakers For Men  ₹399  60% off  \n",
       "4   Combo Pack of 4 Casual Sneakers With Sneakers ...  ₹474  76% off  \n",
       "..                                                ...   ...      ...  \n",
       "95                                   Sneakers For Men  ₹298  70% off  \n",
       "96                                   Sneakers For Men  ₹474  52% off  \n",
       "97  Rockstyle Trending Multicolor Ultralight canva...  ₹398  60% off  \n",
       "98  Men's Combo Pack of 02 Shoes for Men Casual Sn...  ₹470  76% off  \n",
       "99                                   Sneakers For Men  ₹664  71% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "awe = pd.DataFrame({\n",
    "        \"BRAND\": Brand[:100],\n",
    "         \"PRODUCT_DESC\": Product_Description[:100],\n",
    "         \"PRICE\": Price[:100],\n",
    "         \"DISCOUNT\": discount[:100]\n",
    "    })\n",
    "awe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\n",
    "Scrape data for first 10 Laptops you find when you visit flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = []\n",
    "prices = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list(range(1,2))\n",
    "for page in pages:\n",
    "    req = requests.get(\"https://www.flipkart.com/search?q=laptops&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page={}\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup4 = BeautifulSoup(req,'html.parser')\n",
    "    desc = soup4.find_all('div' , class_='_4rR01T')\n",
    "    for i in range(len(desc)):\n",
    "        descriptions.append(desc[i].text)\n",
    "    len(descriptions)\n",
    "    price = soup4.find_all('div',class_='_30jeq3 _1_WHN1') \n",
    "    for i in range(len(price)):\n",
    "        prices.append(price[i].text)\n",
    "        len(prices)\n",
    "\n",
    "    rating = soup4.find_all('div',class_='_3LWZlK') \n",
    "    for i in range(len(rating)):\n",
    "        ratings.append(rating[i].text)\n",
    "        len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>RATING</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo Ideapad Slim 3 Celeron Dual Core - (4 G...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>₹23,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acer Aspire 7 Ryzen 5 Hexa Core 5500U - (8 GB/...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>₹55,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP 15s Core i3 11th Gen - (8 GB/1 TB HDD/Windo...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>₹38,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lenovo Ideapad Slim 3i Core i3 10th Gen - (8 G...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>₹35,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>APPLE MacBook Air M1 - (8 GB/256 GB SSD/Mac OS...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>₹92,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>APPLE MacBook Air Core i5 5th Gen - (8 GB/128 ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>₹67,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lenovo Ideapad S145 Core i3 10th Gen - (4 GB/2...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>₹33,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASUS VivoBook 14 Ryzen 5 Hexa Core 4500U - (8 ...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>₹49,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>msi GF63 Thin Core i5 9th Gen - (8 GB/512 GB S...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>₹54,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo Ideapad 3 Core i5 10th Gen - (8 GB/1 TB...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>₹43,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE RATING    PRICE\n",
       "0  Lenovo Ideapad Slim 3 Celeron Dual Core - (4 G...    4.3  ₹23,990\n",
       "1  acer Aspire 7 Ryzen 5 Hexa Core 5500U - (8 GB/...    4.5  ₹55,990\n",
       "2  HP 15s Core i3 11th Gen - (8 GB/1 TB HDD/Windo...    4.7  ₹38,490\n",
       "3  Lenovo Ideapad Slim 3i Core i3 10th Gen - (8 G...    4.3  ₹35,990\n",
       "4  APPLE MacBook Air M1 - (8 GB/256 GB SSD/Mac OS...    4.6  ₹92,900\n",
       "5  APPLE MacBook Air Core i5 5th Gen - (8 GB/128 ...    4.7  ₹67,990\n",
       "6  Lenovo Ideapad S145 Core i3 10th Gen - (4 GB/2...    4.1  ₹33,990\n",
       "7  ASUS VivoBook 14 Ryzen 5 Hexa Core 4500U - (8 ...    4.4  ₹49,990\n",
       "8  msi GF63 Thin Core i5 9th Gen - (8 GB/512 GB S...    4.5  ₹54,990\n",
       "9  Lenovo Ideapad 3 Core i5 10th Gen - (8 GB/1 TB...    4.1  ₹43,990"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dwe = pd.DataFrame({\n",
    "        \"TITLE\": descriptions[:10],\n",
    "         \"RATING\": ratings[:10],\n",
    "         \"PRICE\": prices[:10],\n",
    "    })\n",
    "dwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
